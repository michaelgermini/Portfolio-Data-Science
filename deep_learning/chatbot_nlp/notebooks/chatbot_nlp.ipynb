{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chatbot NLP â€“ Intents Classification + Q&A\n",
        "\n",
        "This notebook fine-tunes a lightweight text classifier for intents and demonstrates a simple retrieval for answers.\n",
        "\n",
        "Data format: CSV with `text,intent,answer` under `../data/intents_sample.csv`.\n",
        "\n",
        "Run in project root:\n",
        "\n",
        "```bash\n",
        ".venv\\Scripts\\activate\n",
        "jupyter lab deep_learning/chatbot_nlp/notebooks/chatbot_nlp.ipynb\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "DATA_PATH = Path(__file__).resolve().parents[1] / 'data' / 'intents_sample.csv'\n",
        "\n",
        "# Load data\n",
        "assert DATA_PATH.exists(), f\"Data file not found: {DATA_PATH}\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df.dropna(subset=['text','intent']).copy()\n",
        "labels = sorted(df['intent'].unique())\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "df['label_id'] = df['intent'].map(label2id)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'].tolist(), df['label_id'].tolist(), test_size=0.2, stratify=df['label_id'], random_state=42)\n",
        "\n",
        "# Tokenizer & model\n",
        "checkpoint = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
        "\n",
        "def encode_batch(texts, labels):\n",
        "    enc = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    enc['labels'] = torch.tensor(labels)\n",
        "    return enc\n",
        "\n",
        "train_enc = encode_batch(X_train, y_train)\n",
        "test_enc = encode_batch(X_test, y_test)\n",
        "\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc): self.enc = enc\n",
        "    def __len__(self): return self.enc['input_ids'].shape[0]\n",
        "    def __getitem__(self, idx): return {k: v[idx] for k, v in self.enc.items()}\n",
        "\n",
        "train_ds = SimpleDataset(train_enc)\n",
        "test_ds = SimpleDataset(test_enc)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='outputs',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    disable_tqdm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate\n",
        "preds = trainer.predict(test_ds).predictions\n",
        "pred_ids = preds.argmax(axis=1)\n",
        "print(classification_report(y_test, pred_ids, target_names=labels))\n",
        "print(confusion_matrix(y_test, pred_ids))\n",
        "\n",
        "# Simple retrieval for answers\n",
        "answer_map = dict(zip(df['intent'], df.get('answer', ['']*len(df))))\n",
        "\n",
        "def answer_for_intent(intent: str) -> str:\n",
        "    return answer_map.get(intent, '')\n",
        "\n",
        "# Demo\n",
        "sample = \"what are your opening hours?\"\n",
        "inputs = tokenizer(sample, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "pred = logits.argmax(dim=1).item()\n",
        "intent = id2label[pred]\n",
        "print({'text': sample, 'intent': intent, 'answer': answer_for_intent(intent)})\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
